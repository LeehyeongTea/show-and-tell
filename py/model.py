# -*- coding: utf-8 -*-
"""model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z8jdByYmWChhyVu-T4Mi3ys44UKUYyAZ
"""

import numpy as np
import h5py
import os
from tensorflow.keras.layers import Dense, LSTM, Input, Embedding, Dropout,BatchNormalization,Lambda, Add
from tensorflow.keras.optimizers import Adam, RMSprop
from tensorflow.keras.applications import DenseNet121
from tensorflow.keras.models import Model

from tensorflow.keras import regularizers
from tensorflow.keras import backend as K
from tensorflow.keras.models import load_model

from tensorflow.keras.utils import plot_model
import pickle


def make_model( unit_size, max_len,vocab_size,dropout_rate, reg):
  input_img = Input(shape = (2048,))
  dropout_img = Dropout(dropout_rate)(input_img)
  img_fc = Dense(unit_size,kernel_regularizer = regularizers.l2(reg),activation='relu')(dropout_img)
  img_fc = BatchNormalization()(img_fc)

  input_text = Input(shape = (max_len,))
  embeding_layer = Embedding(vocab_size, unit_size, mask_zero = True)(input_text)
  dropout_text = Dropout(dropout_rate)(embeding_layer)
  lstm = LSTM(unit_size,dropout=dropout_rate, activation = 'relu')(dropout_text)

  decoder = Add()([img_fc,lstm])
  decoder = BatchNormalization()(decoder)
  fc = Dense(unit_size,kernel_regularizer = regularizers.l2(reg), activation='relu')(decoder)
  fc = BatchNormalization()(fc)
  outputs = Dense(vocab_size,kernel_regularizer = regularizers.l2(reg), activation='softmax')(fc)
  model = Model(inputs=[input_img, input_text], outputs = outputs)
  model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics=['acc'])
  return model
  
def get_feature_x_y(features,seq_x,seq_y,elem):
  f = features[elem][:]
  x = seq_x[elem][:]
  y = seq_y[elem][:]
  return f,x,y  

def sque(F,X,Y):
  return np.array(F).squeeze(),np.array(X).squeeze(),np.array(Y).squeeze()


def training(model,batch_size,sorted_list,feature,seq_X,seq_Y,index,
               batch_loss,batch_acc):
   
  F = list()
  X = list()
  Y = list()
  for elem in sorted_list:  
    f,x,y = get_feature_x_y(feature,seq_X,seq_Y,elem)  
    for i in range(len(y)):
      F.append(f)
      X.append(x[i])
      Y.append(y[i])
    index += 1
    t_f,t_x,t_y = sque(F,X,Y)
    if index % batch_size == 0:
      t_loss, t_acc = model.train_on_batch([t_f,t_x], t_y)
      print("batch_train_loss : " + str(t_loss) +"  batch_train_acc : " +str(t_acc))
      batch_loss.append(t_loss)
      batch_acc.append(t_acc)
      F.clear()
      X.clear()
      Y.clear()


    #남은 부분
  if index % batch_size != 0:
    t_loss,t_acc = model.train_on_batch([t_f,t_x],t_y)
    print("batch_train_loss : " + str(t_loss) +"   batch_train_acc : " +str(t_acc))
    batch_loss.append(t_loss)
    batch_acc.append(t_acc)
    
def test(model,batch_size,sorted_list,feature,seq_X,seq_Y,index,
               batch_loss,batch_acc):
   
  F = list()
  X = list()
  Y = list()
  for elem in sorted_list:  
    f,x,y = get_feature_x_y(feature,seq_X,seq_Y,elem)  
    for i in range(len(y)):
      F.append(f)
      X.append(x[i])
      Y.append(y[i])
    index += 1
    t_f,t_x,t_y = sque(F,X,Y)
    if index % batch_size == 0:
      v_loss, v_acc = model.test_on_batch([t_f,t_x], t_y)
      print("batch_val_loss : " + str(v_loss) +"  batch_val_acc : " +str(v_acc))
      batch_loss.append(v_loss)
      batch_acc.append(v_acc)
      F.clear()
      X.clear()
      Y.clear()


    #남은 부분
  if index % batch_size != 0:
    v_loss,v_acc = model.test_on_batch([t_f,t_x],t_y)
    print("batch_val_loss : " + str(v_loss) +" batch_val_acc : " +str(v_acc))    
    batch_loss.append(v_loss)
    batch_acc.append(v_acc)
    
if __name__ == "__main__" :
  saved_model_path=input()
  #model이 저장될 장소 입력
  saved_data_path = input()
  #데이터가 저장된 장소 입력
  data_h5_paths = os.path.join(saved_data_path, 'needs.hdf5')
  needs = h5py.File(data_h5_paths, 'r')

  train_feature_path = needs['train_feature_path'][()]
  val_feature_path = needs['val_feature_path'][()]

  train_seq_path_X = needs['train_seq_path_X'][()]
  val_seq_path_X = needs['val_seq_path_X'][()]

  train_seq_path_Y = needs['train_seq_path_Y'][()]
  val_seq_path_Y = needs['val_seq_path_Y'][()]

  max_len = needs['max_len'][()]
  vocab_size= needs['vocab_size'][()]
  
  req_train_list_path = needs['req_train_list_path'][()]
  req_val_list_path = needs['req_val_list_path'][()]
  with open(req_train_list_path, 'rb') as handle:
    train_list = pickle.load(handle)
  with open(req_val_list_path,'rb') as handle:
    val_list = pickle.load(handle)


  model = make_model(512, max_len,vocab_size,0.5,1e-4)
  print(model.summary())
  
  

  train_features = h5py.File(train_feature_path, 'r')
  train_seq_x = h5py.File(train_seq_path_X, 'r')
  train_seq_y = h5py.File(train_seq_path_Y, 'r')
  

  val_features = h5py.File(val_feature_path, 'r')
  val_seq_x = h5py.File(val_seq_path_X,'r')
  val_seq_y = h5py.File(val_seq_path_Y,'r') 
  

  train_loss =[]
  train_acc =[]
  val_loss = []
  val_acc = []

  for epoch in range(0,40):
    #트레이닝
    batch_train_loss = []
    batch_train_acc = []

    batch_val_loss = []
    batch_val_acc = []
    index = 0
    #train
    training(model,100,train_list,train_features,train_seq_x,train_seq_y,index,
               batch_train_loss,batch_train_acc)
    train_loss.append(np.mean(batch_train_loss))
    train_acc.append(np.mean(batch_train_acc))
    #vaild
    test(model,100,val_list,val_features,val_seq_x,val_seq_y,index,
               batch_val_loss,batch_val_acc)
    val_loss.append(np.mean(batch_val_loss))
    val_acc.append(np.mean(batch_val_acc))
    

    print('epoch {0:4d} train acc {1:0.3f} loss {2:0.3f} val acc {3:0.3f} loss {4:0.3f}'.
          format(epoch, np.mean(batch_train_acc), np.mean(batch_train_loss), np.mean(batch_val_acc), np.mean(batch_val_loss)))
    
    path=os.path.join(saved_model_path, 'model_{0:02d}_vacc_{1:0.3f}_vloss_{2:0.3f}_acc{3:0.3f}_loss{4:0.3f}.h5'.
                      format(epoch+1,np.mean(batch_val_acc), np.mean(batch_val_loss),np.mean(batch_train_acc),np.mean(batch_train_loss)))
    model.save(path)
  train_features.close()
  train_seq_x.close()
  train_seq_y.close()

  val_features.close()
  val_seq_x.close()
  val_seq_y.close()