{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "evalutation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyORPz6eh6HfE1ZdMGq9mz+m"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6idBMI-E0SL",
        "outputId": "1c51fb24-9ca1-4d39-bc94-186afd3a4cdc"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive',force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1_Q570o-XB4",
        "outputId": "64255ea1-ed51-461a-ca0f-3e517966ddb0"
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer \n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "import h5py\n",
        "import re\n",
        "\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "import pickle\n",
        "\n",
        "def create_caption_greedy(model, tokenizer, feature, max_len):\n",
        "  start = 'sq'\n",
        "  for i in range(max_len):\n",
        "    seq = tokenizer.texts_to_sequences([start])[0]\n",
        "    seq = pad_sequences([seq],maxlen = max_len)\n",
        "\n",
        "    pred = model.predict([feature, seq],verbose = 0)\n",
        "    \n",
        "    pred = np.argmax(pred)\n",
        "    pred_word = 'eq'\n",
        "\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "      if i == pred:\n",
        "        pred_word = word\n",
        "\n",
        "    start = start + ' '+ pred_word\n",
        "    if pred_word == 'eq':\n",
        "      break\n",
        "  return start\n",
        "\n",
        "\n",
        "\n",
        "def create_caption_beam(model, tokenizer, feature, max_len, k):\n",
        "  start = 'sq'\n",
        "  start = tokenizer.texts_to_sequences([start])[0]\n",
        "  depth_seq=[]\n",
        "  for word, num in tokenizer.word_index.items():\n",
        "    if word == 'eq':\n",
        "      end = num\n",
        "  #end일때 더이상 따지지 않기 위해 end를 찾아놓음\n",
        "  sequences = [[start, 0.0]]\n",
        "  max_len_flag=list()\n",
        "  end_flag =0\n",
        "\n",
        "  depth_seq=[]\n",
        "  while len(sequences[0][0]) < max_len :\n",
        "    all_candidate = list()\n",
        "    depth =1\n",
        "    for sequence in sequences:\n",
        "      seq = pad_sequences([sequence[0]],maxlen = max_len)\n",
        "      \n",
        "\n",
        "      all_pred = model.predict([feature, seq],verbose = 0)\n",
        "\n",
        "      candidate_preds = np.argsort(all_pred[0])[-k:]\n",
        "\n",
        "      for candidate in candidate_preds: \n",
        "        get_cap = sequence[0][:]\n",
        "        score = sequence[1]\n",
        "        get_cap.append(candidate)\n",
        "        score = score+np.log(all_pred[0][candidate]+1e-10)\n",
        "        if get_cap[-1] == end :\n",
        "          #depth만큼 나눠 평균을 구해준다.\n",
        "          depth_seq.append([get_cap,score/depth])\n",
        "          continue\n",
        "        else :  \n",
        "          all_candidate.append([get_cap,score])\n",
        "    sequences = all_candidate\n",
        "    sequences = sorted(sequences, key = lambda I: I[1])\n",
        "    sequences = sequences[-k:]\n",
        "\n",
        "  depth_seq = sorted(depth_seq, key = lambda l: l[1])\n",
        "  depth_seq = depth_seq[-1][0]\n",
        "  #최고점수 시퀀스 찾음\n",
        "  text = ''\n",
        "  for t in depth_seq:\n",
        "    for word,i in tokenizer.word_index.items():\n",
        "      if i==t:\n",
        "        if word != 'eq' or word != 'sq':\n",
        "          text = text+' '+word\n",
        "        elif word == 'eq':\n",
        "          break\n",
        "\n",
        "  return text\n",
        "\n",
        "\n",
        "\n",
        "def Bleu_Score(model, seq,sortedList, feature, tokenizer,saved_path, max_len):\n",
        "  y = list()\n",
        "  pred = list()\n",
        "\n",
        "  for id in sortedList:\n",
        "    f_arr = np.array(feature[id][:])\n",
        "    generated = create_caption_greedy(model, tokenizer, f_arr, max_len)\n",
        "    target_words = [line.split() for line in seq[id]]\n",
        "    y.append(target_words)\n",
        "    pred.append(generated.split())\n",
        "\n",
        "  print('greedy')\n",
        "  print('BLEU-1: ' + str(corpus_bleu(y,pred,weights=(1.0,0,0,0))))\n",
        "  print('BLEU-2: ' + str(corpus_bleu(y,pred,weights=(0.5,0.5,0,0))))\n",
        "  print('BLEU-3: ' + str(corpus_bleu(y,pred,weights=(0.3,0.3,0.3,0))))\n",
        "  print('BLEU-4: ' + str(corpus_bleu(y,pred,weights=(0.25, 0.25, 0.25, 0.25))))\n",
        "  with open(saved_path, 'w') as f:\n",
        "    f.write('greedy\\n')\n",
        "    f.write('BLEU-1: ' + str(corpus_bleu(y,pred,weights=(1.0,0,0,0)))+'\\n')\n",
        "    f.write('BLEU-2: ' + str(corpus_bleu(y,pred,weights=(0.5,0.5,0,0)))+'\\n')\n",
        "    f.write('BLEU-3: ' + str(corpus_bleu(y,pred,weights=(0.3,0.3,0.3,0)))+'\\n')\n",
        "    f.write('BLEU-4: ' + str(corpus_bleu(y,pred,weights=(0.25, 0.25, 0.25, 0.25)))+'\\n')\n",
        "  \n",
        "def k_beam_Bleu_Score(model, seq,sortedList, feature, tokenizer,saved_path, max_len, k):\n",
        "  y = list()\n",
        "  pred = list()\n",
        "\n",
        "  for id in sortedList:\n",
        "    f_arr = np.array(feature[id][:])\n",
        "    generated = create_caption_beam(model, tokenizer, f_arr, max_len, k)\n",
        "    target_words = [line.split() for line in seq[id]]\n",
        "    y.append(target_words)\n",
        "    pred.append(generated.split())\n",
        "\n",
        "  print('beam')\n",
        "  print('BLEU-1: ' + str(corpus_bleu(y,pred,weights=(1.0,0,0,0))))\n",
        "  print('BLEU-2: ' + str(corpus_bleu(y,pred,weights=(0.5,0.5,0,0))))\n",
        "  print('BLEU-3: ' + str(corpus_bleu(y,pred,weights=(0.3,0.3,0.3,0))))\n",
        "  print('BLEU-4: ' + str(corpus_bleu(y,pred,weights=(0.25, 0.25, 0.25, 0.25))))\n",
        "  with open(saved_path, 'w') as f:\n",
        "    f.write(str(k)+'beam\\n')\n",
        "    f.write('BLEU-1: ' + str(corpus_bleu(y,pred,weights=(1.0,0,0,0)))+'\\n')\n",
        "    f.write('BLEU-2: ' + str(corpus_bleu(y,pred,weights=(0.5,0.5,0,0)))+'\\n')\n",
        "    f.write('BLEU-3: ' + str(corpus_bleu(y,pred,weights=(0.3,0.3,0.3,0)))+'\\n')\n",
        "    f.write('BLEU-4: ' + str(corpus_bleu(y,pred,weights=(0.25, 0.25, 0.25, 0.25)))+'\\n')\n",
        "  \n",
        "    \n",
        "  \n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  \n",
        "  \n",
        "  base_directory = '/content/gdrive/My Drive/Colab Notebooks/showandtell'\n",
        "  saved_data_path = os.path.join(base_directory,'data')\n",
        "  data_h5_paths = os.path.join(saved_data_path, 'needs.hdf5')\n",
        "  needs = h5py.File(data_h5_paths, 'r')\n",
        "\n",
        "  test_dataset_list_path = needs['test_code_path'][()]\n",
        "  \n",
        "  test_feature_path = needs['test_feature_path'][()]\n",
        "\n",
        "  test_seq_path_X = needs['test_seq_path_X'][()]\n",
        "  test_seq_path_Y = needs['test_seq_path_Y'][()]\n",
        "  \n",
        "  tokenpath = needs['token_path'][()]\n",
        "  max_len = needs['max_len'][()]\n",
        "  vocab_size= needs['vocab_size'][()]\n",
        "  \n",
        "  req_token_path = needs['req_token_path'][()]\n",
        "  req_test_list_path = needs['req_test_list_path'][()]\n",
        "  req_seq_path = needs['req_seq_path'][()]\n",
        "  \n",
        "  model_path = os.path.join(base_directory,'real_saved_model','model_39_vacc_0.372_vloss_3.722_acc0.538_loss2.069.h5')\n",
        "  saved_path = os.path.join(base_directory,'data','greedy.txt')\n",
        "  beam_saved_path = os.path.join(base_directory,'data','k=3beam.txt')\n",
        "  \n",
        "  \n",
        "  model = load_model(model_path)\n",
        "\n",
        "  with open(req_token_path, 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)\n",
        "  with open(req_test_list_path, 'rb') as handle:\n",
        "    test_list = pickle.load(handle)\n",
        "  with open(req_seq_path, 'rb') as handle:\n",
        "    sequence = pickle.load(handle)\n",
        "  with h5py.File(test_feature_path,'r') as hf:\n",
        "    model = load_model(model_path)\n",
        "    Bleu_Score(model, sequence,test_list, hf, tokenizer, saved_path, max_len)\n",
        "    k_beam_Bleu_Score(model, sequence,test_list, hf, tokenizer,beam_saved_path, max_len, 3)\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "greedy\n",
            "BLEU-1: 0.5714937980366791\n",
            "BLEU-2: 0.3440705692607763\n",
            "BLEU-3: 0.244745379753475\n",
            "BLEU-4: 0.12625593506226632\n",
            "beam\n",
            "BLEU-1: 0.6128750481633843\n",
            "BLEU-2: 0.3630172269816211\n",
            "BLEU-3: 0.2559594519059041\n",
            "BLEU-4: 0.13715272634208575\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}